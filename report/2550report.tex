% -----
% COMP2550/COMP3130 Warmup Project Report
% CHRISTOPHER CLAOUE-LONG
% -----

% DELIVERABLES:
%You need to submit your source code (in a tarball) and a two page technical report (as a PDF). Your report should include a description of your features, some example results (pictures), a numerical analysis of your algorithm, and answers to the following questions:
%¥ What is the performance of your algorithm on the training set compared to the test set? Is this result expected?
%¥ Why is it important to evaluate pixelwise accuracy instead of accuracy on the superpixels? ¥ What do you think is more important, the features or the machine learning classifier?
%The software and two page report are due at 11:59pm on 22 March, 2012.
% -
% - DOCUMENT GEOMETRY SETUP
\documentclass[11pt,a4paper]{article}
\usepackage{geometry}
\geometry{margin=20mm}
\usepackage{lastpage}
\makeatletter \renewcommand{\@oddfoot}{\hfil Page \thepage\ of \pageref{LastPage} \hfil} \makeatother
% -
% - SECTION FORMATTING
\renewcommand \thepart{\Roman{part}}
\renewcommand \thesection{\arabic{section}}
\renewcommand \thesubsection{\arabic{section}.\arabic{subsection}}
\renewcommand \thesubsubsection{\arabic{section}.\arabic{subsection}.\arabic{subsubsection}}
% -
% - FONT
\usepackage{amsmath, amsthm, amssymb,graphicx,epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\usepackage[sc]{mathpazo}
\linespread{1.05}
\usepackage[T1]{fontenc}
\usepackage[bitstream-charter]{mathdesign}
% -
% - MISC. PACKAGES
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}\usepackage{tikz} \usepackage{qtree, tikz-qtree, lineno}
\renewcommand\linenumberfont{\normalfont\sffamily}
\usepackage{datetime, multicol, verbatim, ulem, alltt, multirow, hyperref}
\hypersetup{
colorlinks,
citecolor=black,		% - Citation colour
filecolor=black,		% - File colour
linkcolor=black,		% - Link colour
urlcolor=black		% - URL colour
}
\urlstyle{same}
% -
% -
% - MISC. SYMBOLS AND COMMANDS
% - Thick horizontal blue line
\newcommand{\Hrule}{\textcolor{blue}{\rule{\linewidth}{0.5mm}}}
\newcommand{\HUGEBOLD}[1]{\textbf{\Huge{#1}}}
% -----
\begin{document}
% -----
% - Title
\begin{center}
\Hrule\\
\textbf{\Huge COMP2550/COMP3130 ANU\\Warmup Project Report}\\
\textbf{\\\large Christopher Claou\'e-Long 
(\href{mailto:u5183532@anu.edu.au}{\textit{\underline{\smash{u5183532@anu.edu.au}}}})\\
Jimmy Lin 
(\href{mailto:u5223173@anu.edu.au}{\textit{\underline{\smash{u5223173@anu.edu.au}}}})\\}
\Hrule
\end{center}
% -
% -
\begin{multicols}{2}
\section{Introduction and Overview}
The field of computer vision is often thought of as a diverse topic, however in reality there are three major paths to which each project can be linked.  The first two, scene categorisation and object detection, aim to provide a summary of a scene using tags and to detect discrete objects within an image respectively, however do not deliver an accurate object outline nor consider the image as a whole such as what happens in one portion of human vision.  The task at hand for this warmup project was to further the third path of annotating an entire image at the pixel level, by considering a unified cluster of pixels as a superpixel and extracting a set of useful features to check against using the open source Darwin framework for machine learning and the MSRC dataset.  The desired outcome was a feature vector that allowed machine learning algorithms to classify images with over 50\% accuracy.

\section{Method}

The first few features implemented were the average red, green and blue values of the superpixel, since this would begin to differentiate between extremely different superpixels, such as those describing sky and grass.  Adding in the standard deviation of these colours could also help differentiate between differently textured models.  We also tried out getting the difference and absolute difference between the red, green and blue average values across the superpixel to further bring out features dependent on the superpixel composition. Other features tested include the average gradient in the superpixel, its centre to know the general location within the image (to help differentiate between sky and water for example), and taking into consideration the differences between the superpixel's immediate neighbours and itself, intuitively approximating the outlines of objects to help the classifier decide what superpixels are more likely to be when in close proximity to superpixels of another type.

\begin{comment}
First of all, we consider the texture of one superpixel. One possible implementation is to respectively discretize the lightness of marginal RGB value (9 for each interval length) and assign each pixel to those intervals based on their RGB lightness. The strength of this method is the tremendous informativeness and it does result in a satisfying accuracy increase (directly to arround 0.43) . However, that discretization method occupies too many dimensions of feature vector (87 attributes) to the extent that the classifier was badly sensitive to other features. Alternatively,we attempt the mean and standard deviation of absolute marginal RGB value (only 6 attributes). Expectedly, it produces a considerable rise with occupying fewer dimension of feature vector. (It will be demonstrated latter why occupation of less dimensions is vital.) 

The second significant set of features is the mean and standard deviation of relative lightness of one pixel's marginal RGB value, that is, the lightness difference between the pairs of Red and Green, Green and Blue, Blue and Red. 

On top of that, some other features that lead to comparatively slight accuracy advances are added into the our feature vector as well. For instance, the average smoothness counts the mean of RGB lightness difference of all pixels to their neighbour pixels. And location of one superpixel, more specifically, evalutates the mean and standard deviation of x,y coordinate of one pixel in its located image. The idea of adding this set of feature our final selection comes from the semantic perceptions that the higher stuff is more likely to be sky and that the lower stuff is more likely be ground or grass. 

What we discussed before is all about the features of one superpixel itself. To further enhance the accuracy, we may need to, when forming the feature vector of one superpixel, take into consideration the brief information of neighboured superpixel. That is an intuitively meaningful approach because even human cannot recognize a small region without perceiving its surroundings. Yet, the experiments we have accomplished till now does not indicate the effectiveness of that approach. One possible reason is that the features we pick out for this approach provide little information regarding the surrounded environment of one superpixel. By the way, we apply some algorithm optimizations to reduce the time cost for training of classifier with neighbour superpixel detection, from about 6 hours to 25 minutes. One way to fulfill this progress is to replace linear list with hash set to minimize the time for membership evaluation. 
\end{comment}

\section{Results}
Pictures, maybe accuracy tables with different features implemented?

\section{Discussion}
numerical analysis of your algorithm - accuracy, number of features, efficiency/running-time estimate, see how this changes with certain algorithm parameters (eg number of features, depth of decision tree, broken down by class, etc)
What is the performance of your algorithm on the training set compared to the test set? Is this result expected?\\
Why is it important to evaluate pixelwise accuracy instead of accuracy on the superpixels?\\
What do you think is more important, the features or the machine learning classifier?\\
Interpretation of results, what went well, what went wrong (overfitting?), what could be done better.

\section{References}
S. Gould. DARWIN: A Framework for Machine Learning and Computer Vision Research and Development. In \textit{Journal of Machine Learning Research (JMLR)}, 2012.\\
K. Park and S. Gould. On Learning Higher-Order Consistency Potentials for Multi-class Pixel Labeling. In \textit{Proceedings of the European Conference on Computer Vision (ECCV)}, 2012.

\end{multicols}
\vfill\Hrule
\end{document}
% -----
% END OF LINE
% -----
