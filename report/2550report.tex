% -----
% COMP2550/COMP3130 Warmup Project Report
% CHRISTOPHER CLAOUE-LONG
% -----

% DELIVERABLES:
%You need to submit your source code (in a tarball) and a two page technical report (as a PDF). Your report should include a description of your features, some example results (pictures), a numerical analysis of your algorithm, and answers to the following questions:
%¥ What is the performance of your algorithm on the training set compared to the test set? Is this result expected?
%¥ Why is it important to evaluate pixelwise accuracy instead of accuracy on the superpixels? ¥ What do you think is more important, the features or the machine learning classifier?
%The software and two page report are due at 11:59pm on 22 March, 2012.
% -
% - DOCUMENT GEOMETRY SETUP
\documentclass[11pt,a4paper]{article}
\usepackage{geometry}
\geometry{margin=20mm}
\usepackage{lastpage}
\makeatletter \renewcommand{\@oddfoot}{\hfil Page \thepage\ of \pageref{LastPage} \hfil} \makeatother
% -
% - SECTION FORMATTING
\renewcommand \thepart{\Roman{part}}
\renewcommand \thesection{\arabic{section}}
\renewcommand \thesubsection{\arabic{section}.\arabic{subsection}}
\renewcommand \thesubsubsection{\arabic{section}.\arabic{subsection}.\arabic{subsubsection}}
% -
% - FONT
\usepackage{amsmath, amsthm, amssymb,graphicx,epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\usepackage[sc]{mathpazo}
\linespread{1.05}
\usepackage[T1]{fontenc}
\usepackage[bitstream-charter]{mathdesign}
% -
% - MISC. PACKAGES
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}\usepackage{tikz} \usepackage{qtree, tikz-qtree, lineno}
\renewcommand\linenumberfont{\normalfont\sffamily}
\usepackage{datetime, multicol, verbatim, ulem, alltt, multirow, hyperref}
\hypersetup{
colorlinks,
citecolor=black,		% - Citation colour
filecolor=black,		% - File colour
linkcolor=black,		% - Link colour
urlcolor=black		% - URL colour
}
\urlstyle{same}
% -
% -
% - MISC. SYMBOLS AND COMMANDS
% - Thick horizontal blue line
\newcommand{\Hrule}{\textcolor{blue}{\rule{\linewidth}{0.5mm}}}
\newcommand{\HUGEBOLD}[1]{\textbf{\Huge{#1}}}
% -----
\begin{document}
% -----
% - Title
\begin{center}
\Hrule\\
\textbf{\Huge COMP2550/COMP3130 ANU\\Warmup Project Report}\\
\textbf{\\\large Christopher Claou\'e-Long 
(\href{mailto:u5183532@anu.edu.au}{\textit{\underline{\smash{u5183532@anu.edu.au}}}})\\
Jimmy Lin 
(\href{mailto:u5223173@anu.edu.au}{\textit{\underline{\smash{u5223173@anu.edu.au}}}})\\}
\Hrule
\end{center}
% -
% -
\begin{multicols}{2}
\section{Introduction and Overview}
The field of computer vision is often thought of as a diverse topic, however in reality there are three major paths to which each project can be linked.  The first two -- scene categorisation and object detection -- aim to provide a summary of a scene using tags or bounding boxes around discrete objects, however do not deliver an accurate object outline nor consider the image as a whole like what happens in human vision.

The task for this warmup project was to further the third path of research: annotating an entire image at the pixel level to describe the entire scene.  This would be done by considering a unified cluster of pixels (a superpixel) and extracting a set of useful features to check against using the open source Darwin framework for machine learning and the MSRC dataset.  The desired outcome was a feature vector that allowed machine learning algorithms to classify superpixels in images with at least 50\% accuracy.

\section{Method}

The first few features implemented were the average red, green and blue values of the superpixel, since this would begin to differentiate between extremely different superpixels such as those describing sky and grass.  Adding in the standard deviation of these colours could also help differentiate between differently textured models.  Calculating the difference and absolute difference between the red, green and blue average values across the superpixel to further bring out features dependent on the superpixel composition may also help the classifier decide what label to apply. Other features tested include the average gradient % the "smoothness"
 in the superpixel, its centre to know the general location within the image (to help differentiate between sky and water for example), and taking into consideration the differences between the superpixel's immediate neighbours and itself using a hash set for speed to intuitively approximate the outlines of objects to help the classifier decide what superpixels are more likely to be when in close proximity to superpixels of another type.

\begin{comment}
the average smoothness counts the mean of RGB lightness difference of all pixels to their neighbour pixels. 

And location of one superpixel, more specifically, evalutates the mean and standard deviation of x,y coordinate of one pixel in its located image. 

To further enhance the accuracy, we may need to, when forming the feature vector of one superpixel, take into consideration the brief information of neighboured superpixel. 

That is an intuitively meaningful approach because even human cannot recognize a small region without perceiving its surroundings. 

Yet, the experiments we have accomplished till now does not indicate the effectiveness of that approach.

 One possible reason is that the features we pick out for this approach provide little information regarding the surrounded environment of one superpixel. 
 
 By the way, we apply some algorithm optimizations to reduce the time cost for training of classifier with neighbour superpixel detection, from about 6 hours to 25 minutes. 
 
 One way to fulfill this progress is to replace linear list with hash set to minimize the time for membership evaluation. 
\end{comment}

\section{Results}
Pictures, maybe accuracy tables with different features implemented?

\section{Discussion}
While the accuracy is above the desired outcome, the efficiency of the algorithm is quite low when considering neighbours since in memory this is respresented as a linear list.  A way to speed this up is to use a hash set to index the neighbours in O(1) instead of O(n) time.

Accuracy discussion

Where the algorithm fails - animals and objects with outlines of similar appearance especially!

Performance on training set compared to test set, is this difference expected?

Changes when modifying depth of decision tree

Why is it important to evaluate pixelwise accuracy instead of accuracy on the superpixels?  Checking accuracy of a superpixel  is difficult, since it may overlap multiple different may overlap onto different label areas, making it impossible to check consistently whether the superpixel was labelled correctly.  Pixelwise accuracy has no possible overlap, ensuring that the overall accuracy of the machine learning algorithm is measured correctly.

What do you think is more important, the features or the machine learning classifier? Machine learning classifiers can be built in multiple ways - vector distance tests, binary search trees, hash sets, cosine similarity, comparing individual features, considering features as a whole, and so on.  However, these classifers are only as good as the features they are given -- it is therefore crucial to discover a set of features that allows the classifier to differentiate superpixels accurately.


Possible improvements include considering a neighbourhood wider than a superpixel's immediate neighbours (to help differentiate buildings and sky, faces/flowers and surrounds etc), taking into account the texture

\section{References}
S. Gould. DARWIN: A Framework for Machine Learning and Computer Vision Research and Development. In \textit{Journal of Machine Learning Research (JMLR)}, 2012.\\
K. Park and S. Gould. On Learning Higher-Order Consistency Potentials for Multi-class Pixel Labeling. In \textit{Proceedings of the European Conference on Computer Vision (ECCV)}, 2012.

\end{multicols}
\vfill\Hrule
\end{document}
% -----
% END OF LINE
% -----
